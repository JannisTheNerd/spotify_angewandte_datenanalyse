{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5976442",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f1e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/spotify.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "except:\n",
    "    # Achtung der Pfad \"../../../_Daten\" muss existieren.\n",
    "    import kagglehub\n",
    "    import shutil\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"atharvasoundankar/spotify-global-streaming-data-2024\")\n",
    "    \n",
    "    # Get the first file in path\n",
    "    files = os.listdir(path)\n",
    "    if len(files) == 0:\n",
    "        raise ValueError(\"No files found in the downloaded dataset.\")\n",
    "    # Check if the data directory exists, if not create it\n",
    "    if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")\n",
    "\n",
    "    # Move the downloaded file to the desired directory\n",
    "    shutil.move(os.path.join(path, files[0]), data_path)\n",
    "\n",
    "    print(\"Spotifiy dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505963e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a regression line between Release Year and total streams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_name = 'Release Year'\n",
    "y_name = 'Total Streams (Millions)'\n",
    "category_name = 'Country'\n",
    "\n",
    "# Filter the DataFrame to include only the relevant columns\n",
    "df_filtered = df[[x_name, y_name, category_name]].dropna()\n",
    "\n",
    "# Convert the Release Year to numeric\n",
    "df_filtered[x_name] = pd.to_numeric(df_filtered[x_name], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values after conversion\n",
    "df_filtered = df_filtered.dropna()\n",
    "\n",
    "# Fitting regression model\n",
    "X = df_filtered[[x_name]].values.reshape(-1, 1)\n",
    "y = df_filtered[y_name].values.reshape(-1, 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "# Plotting the data and the regression line with confidence intervals\n",
    "\n",
    "sns.lmplot(\n",
    "    data=df_filtered, \n",
    "    x=x_name, \n",
    "    y=y_name,\n",
    "    hue=category_name, \n",
    "    ci= 95,\n",
    "    scatter_kws={'s':12},)\n",
    "plt.title(f'{y_name} vs {x_name} with Regression Line')\n",
    "plt.xlabel(x_name)\n",
    "plt.ylabel(y_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914af34",
   "metadata": {},
   "source": [
    "# Hauptkomponenten Analyse\n",
    "Nach dem festgestellt wurden das die Skip Rate durch mehrere Faktoren beeinflusst werden, soll die folgende Haupkomponenten Analyse aufklären, welche Faktoren am meisten zur Aufklärung der Varianz beitragen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f49287",
   "metadata": {},
   "source": [
    "Zunächst werden die kategorialen Variablen des Datensatzes mit einem One-Hot-Encoding umgeformt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teilung der Test Daten zur Vorhersage der Skip Rate (%) mit Kodierung kategorialer Variablen\n",
    "# Kategoriale Variablen werden automatisch per One-Hot-Encoding kodiert.\n",
    "# Feature- und Zielspalten\n",
    "feature_cols = [\n",
    "    'Country', \n",
    "    'Artist', \n",
    "    'Album', \n",
    "    'Genre', \n",
    "    'Release Year',\n",
    "    'Monthly Listeners (Millions)', \n",
    "    'Total Streams (Millions)',\n",
    "    'Total Hours Streamed (Millions)', \n",
    "    'Avg Stream Duration (Min)',\n",
    "    'Streams Last 30 Days (Millions)'\n",
    "]\n",
    "\n",
    "target_col = 'Skip Rate (%)'\n",
    "\n",
    "# Filtere und bereite die Daten vor\n",
    "df_reg = df[feature_cols + [target_col]].dropna()\n",
    "X = df_reg[feature_cols]\n",
    "y = df_reg[target_col]\n",
    "\n",
    "# Kategoriale Variablen kodieren mit One-Hot-Encoding\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f6e7a",
   "metadata": {},
   "source": [
    "Bevor eine Hauptkomponentenanalyse durchgeführt wird, werden alle Variablen Standertisiert um Vergleichbarkeit zwischen den Variablen herzustellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptkomponentenanalyse (PCA) auf den kodierten Features\n",
    "# Die Features werden vor der PCA standardisiert.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15433487",
   "metadata": {},
   "source": [
    "Zur Selektion der Hauptkomponenten suchen wir zunächst, wie viele Komponenten es brauch um 90% der Varianz der Daten zu erklären. \n",
    "Die gefundenen Komponenten stellen wir nun mit ihrem Eigenwert in einem Scree-Plot dar. An diesem kann ein Signifikatner Sprung identifiziert werden, um zu Entscheiden ab welchen Komponentenanzahl der Cutoff gewählt wird.\n",
    "Visuell auffällig sind die Sprunge zur Zweiten, Zwölften und Dreizehnten Komponente. Alle Komponente zwischen von dem Ersten bis zur 13. haben einen Eigenwert größer als eins und genügen damit dem Kaiser-Kriterium. \n",
    "Da eine weitere Analyse mit nur einem Kriterium reduntant wäre, wird sich für den nächsten signifikanten Knick an der Komponente zwölf entschieden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04aa268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Führe PCA durch, sodass mindestens 90% der Varianz erklärt werden\n",
    "pca = PCA(n_components=0.9, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Scree-Plot: Eigenwerte der Hauptkomponenten\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1, len(pca.explained_variance_)+1), pca.explained_variance_, marker='o')\n",
    "plt.axhline(1, color='red', linestyle='--', label='Grenze = 1')\n",
    "plt.xlabel('Hauptkomponente')\n",
    "plt.ylabel('Eigenwert')\n",
    "plt.title('Scree-Plot der Hauptkomponenten (PCA)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4076723",
   "metadata": {},
   "source": [
    "Die zwölf selektieren Komponeten erkären gemeinsam 45,66% der Varianz der Daten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9513910",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=12, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Erklärte Varianz: {np.sum(pca.explained_variance_ratio_)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb60a9",
   "metadata": {},
   "source": [
    "Nun stellen wir die Ladungen der Variablen auf den Komponenten dar. Dazu wird eine Varimax transformation genutzt um starke Ladungen hervorzuheben und schwache Ladungen zu unterdrücken. \n",
    "An der Resultierenden Übersicht ist auffällig das jede Komponente von einem Album und ihrem jeweilgen Künstler*innen dominiert wird. \n",
    "Die allgemeinerien Variablen, wie das Erscheinungsjahr oder die Monatlichen Höhrer laden tendenziel auf alle Komponenten gleichermaßen. \n",
    "An der Darstellung ist zu beachten, dass alle Werte die unter den Wert von 0,2 fallen ausgeblendet wurden und die Tabelle lesbar zu gestalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotierte Komponentenmatrix (Varimax-Rotation)\n",
    "from factor_analyzer.rotator import Rotator\n",
    "\n",
    "# Varimax-Rotation auf die PCA-Komponentenmatrix anwenden\n",
    "rotator = Rotator(method='varimax')\n",
    "rotated_components = rotator.fit_transform(pca.components_.T).T\n",
    "\n",
    "# Rotierte Komponentenmatrix als DataFrame anzeigen\n",
    "rotated_df = pd.DataFrame(rotated_components, columns=X_encoded.columns, index=[f'PC{i+1}' for i in range(pca.n_components_)]).T\n",
    "\n",
    "# Für jede Variable: Komponente mit höchster Ladung (nach Betrag) bestimmen\n",
    "max_comp = rotated_df.abs().idxmax(axis=1)\n",
    "max_val = rotated_df.abs().max(axis=1)\n",
    "comp_num = max_comp.str.extract(r'(\\d+)').astype(int)[0]\n",
    "\n",
    "# Sortierindex erstellen: erst nach Komponente, dann nach Höhe der Ladung (absteigend)\n",
    "sort_idx = pd.DataFrame({'comp': comp_num, 'val': max_val})\n",
    "sort_idx = sort_idx.sort_values(['comp', 'val'], ascending=[True, False]).index\n",
    "rotated_df_sorted = rotated_df.loc[sort_idx]\n",
    "\n",
    "# Werte < 0.08 ausblenden\n",
    "rotated_df_masked = rotated_df_sorted.map(lambda x: x if abs(x) >= 0.2 else \"\")\n",
    "\n",
    "# Trick um das gesamte DataFrame anzuzeigen\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(rotated_df_masked)\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfb88d",
   "metadata": {},
   "source": [
    "# Entscheidungsbaum\n",
    "Als Alternative Methode zur multiplen linearen Regression, welche die Skip Rate beeinflusst, wird ein Regressionsentscheidungsbaum trainiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53268d2",
   "metadata": {},
   "source": [
    "Hierzu teilen wir den Datensatz in 20% Testdaten und 80% Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ae240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitte die Daten in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0aa2f0",
   "metadata": {},
   "source": [
    "Da die optimalen Parameter für das Training eines Entscheidungsbaums mit den gegeben Daten unbekannt ist, wird mit einer Gridsuche systematisch Parameter ausprobiert. Es werden jene Parameter ausgewählt, welche in dem größten Wert des Bestimmtheitsmaßes, R², resultieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1022b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search zur Optimierung der Entscheidungsbaum-Parameter\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 12, None],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1, 0.5],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Beste Parameterkombination:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Bester mittlerer R²-Score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Optional: Ergebnisse als DataFrame anzeigen\n",
    "# results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "# display(results_df[['params', 'mean_test_score', 'std_test_score']].sort_values('mean_test_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f9843",
   "metadata": {},
   "source": [
    "Mit den nun bestimmten Parametern wird ein Modell trainiert, welches für weitere Berechnungen verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a782894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Trainiere den Regressionsbaum\n",
    "reg_tree = DecisionTreeRegressor(**grid_search.best_params_, random_state=42)\n",
    "reg_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6abd3",
   "metadata": {},
   "source": [
    "Um ein Overfitting des Modells zu kontrollieren wird eine Kreuzvalidierung über 5 Holdouts berechnet. Das Ergebnis weißt eine akzeptierbare Streuung auf in an betracht der geringen Datenmenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validierung für den Regressionsentscheidungsbaum\n",
    "splits = 5\n",
    "cv = KFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "\n",
    "# R²-Scores\n",
    "r2_scores = cross_val_score(reg_tree, X_encoded, y, cv=cv, scoring='r2')\n",
    "print(f\"Mittlerer R²-Score ({splits}-fold CV): {np.mean(r2_scores):.3f} ± {np.std(r2_scores):.3f}\")\n",
    "\n",
    "# MSE-Scores (negativ, daher Vorzeichen umdrehen)\n",
    "mse_scores = cross_val_score(reg_tree, X_encoded, y, cv=cv, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "print(f\"Mittlerer MSE ({splits}-fold CV): {(-np.mean(mse_scores)):.3f} ± {np.std(mse_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3c74b",
   "metadata": {},
   "source": [
    "Final wird ein Regressionsbaum trainiert dessen Bestimmtheitsmaß negativ ist. Dies lässt darauf schließen, dass die Model schlechter auf die Daten passt als eine vaagerechte Line. Abschließend ist daher zu folgern, dass eine Vorhersage der Skip Rate mittels eines Regressionentscheidungsbaum nicht möglich ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersage und Bewertung\n",
    "y_pred = reg_tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MSE: {mse:.3f}, R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb52878",
   "metadata": {},
   "source": [
    "# Anhang\n",
    "Im Folgeden sind die Darstellungen der Wichtigkeiten der Variablen innerhalb des Entscheidungsbaums dargestellt. Zu erst werden die Wichtigkeit aller Variablen dargestellt. Darauf Folgenden werden Wichtigkiet der kategorialen Variablen gruppiert dargestellt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Importances anzeigen\n",
    "importances = reg_tree.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Feature Importances (Top 15)\")\n",
    "# Zeige nur die wichtigsten 15 Features an\n",
    "top_n = 15\n",
    "plt.bar(range(top_n), importances[indices][:top_n], align=\"center\")\n",
    "plt.xticks(range(top_n), [X_encoded.columns[i] for i in indices[:top_n]], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Feature-Importances gruppiert nach Ursprungsvariablen anzeigen\n",
    "importances = reg_tree.feature_importances_\n",
    "feature_names = X_encoded.columns\n",
    "\n",
    "# Ursprungsvariablen bestimmen\n",
    "orig_vars = feature_cols\n",
    "\n",
    "# Initialisiert ein Dictionary mit Listen für jede Ursprungsvariable\n",
    "var_to_cols = defaultdict(list)\n",
    "for col in feature_names:\n",
    "    found = False\n",
    "    for var in orig_vars:\n",
    "        if col == var or col.startswith(var + '_'):\n",
    "            var_to_cols[var].append(col)\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        var_to_cols[col].append(col)  # falls neue Spalten auftauchen\n",
    "\n",
    "# Importance pro Ursprungsvariable aufsummieren\n",
    "summed_importances = {var: importances[[feature_names.get_loc(c) for c in cols]].sum() for var, cols in var_to_cols.items()}\n",
    "# Sortieren\n",
    "sorted_vars = sorted(summed_importances, key=summed_importances.get, reverse=True)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Feature Importances (gruppiert nach Ursprungsvariablen)\")\n",
    "plt.bar(range(len(sorted_vars)), [summed_importances[v] for v in sorted_vars], align=\"center\")\n",
    "plt.xticks(range(len(sorted_vars)), sorted_vars, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
